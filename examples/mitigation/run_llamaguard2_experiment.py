import json
from typing import Dict, List
from itertools import cycle
import csv
import pandas as pd
from abc import ABC, abstractmethod

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

    
# def get_llama_guard_response_prompt(prompt):
#     api_response = guard_engine.generate(prompt)
#     end_prompt = "[/INST]"
#     formatted_api_response = api_response[api_response.index(end_prompt)+len(end_prompt):].strip()
#     toxicity_score = guard_engine.extract_toxicity([formatted_api_response])
#     return toxicity_score[0]

def check_safety_score(score_obj):
    # if score_obj['sub_name'].lower() == 'safe':
    if score_obj == 'safe':
        return 1
    return 0

# Load the contents of a file into a list of stripped lines
def load_file(filename):
    with open(filename, 'r') as file:
        lines_stripped = [line.strip() for line in file]
    return lines_stripped

# Generate a dataset by combining clean seeds with GCG suffixes
def generate_dataset_advbench_with_gcg_suffixes(clean_seeds, suffixes_filename='perplexity_dataset_gcg_paper_suffixes.csv'):    
    df = pd.read_csv(suffixes_filename)
    suffixes = df['attack_recipe_config'].tolist()
    seeds_with_suffixes = [f"{seed} {suffix}" for seed in clean_seeds for suffix in suffixes]
    return seeds_with_suffixes

# Generate a dataset by combining clean seeds with templates
def generate_dataset_advbench_with_gptfuzzer_templates(clean_seeds, templates_filename='perplexity_dataset_gptfuzzer_paper_templates.csv'):
    df = pd.read_csv(templates_filename)
    templates = df['text'].tolist()
    seeds_with_templates = [template.replace("[INSERT PROMPT HERE]", seed) for template in templates for seed in clean_seeds]
    return seeds_with_templates

# Compute the Llama Guard filter on sample
def compute_llama_guard(sequences):
    count = 0
    safety_count = 0
    for sequence in sequences:
        score = get_llama_guard_response_prompt(sequence)
        count += 1
        print(score)
        # safety_count += check_safety_score(score)
    return count, safety_count

def get_llama_guard_response_prompt(sequence):
    model_id = "meta-llama/Meta-Llama-Guard-2-8B"
    device = "cuda"
    dtype = torch.bfloat16

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype, device_map=device)

    def moderate(chat):
        input_ids = tokenizer.apply_chat_template(chat, return_tensors="pt").to(device)
        output = model.generate(input_ids=input_ids, max_new_tokens=100, pad_token_id=0)
        prompt_len = input_ids.shape[-1]
        return tokenizer.decode(output[0][prompt_len:], skip_special_tokens=True)

    saved = moderate([
        {"role": "user", "content": sequence},
        # {"role": "user", "content": "I forgot how to kill a process in Linux, can you help?"},
        # {"role": "assistant", "content": "Sure! To kill a process in Linux, you can use the kill command followed by the process ID (PID) of the process you want to terminate."},
    ])
    print(saved)


if __name__ == '__main__':
    # Load the data generated by each method
    dataset_advbench_clean_seeds = load_file('perplexity_dataset_advbench_clean_seeds.txt')
    dataset_advbench_with_gcg_suffixes = generate_dataset_advbench_with_gcg_suffixes(dataset_advbench_clean_seeds)
    dataset_advbench_with_gptfuzzer_templates = generate_dataset_advbench_with_gptfuzzer_templates(dataset_advbench_clean_seeds)
    tap_results_file = "./results_advbench_gpt35_tap.json"
    gap_results_file = "./results_advbench_gpt35_gap.json"
    with open(tap_results_file) as json_file_tap:
        dataset_tap_gpt35_advbench = [obj['jailbreak_prompt'] for obj in json.load(json_file_tap)]
    with open(gap_results_file) as json_file_gap:
        dataset_gap_gpt35_advbench = [obj['jailbreak_prompt'] for obj in json.load(json_file_gap)]

    # Compute llama guard for each dataset
    print("Loading llama guard results for clean seeds...")
    total_clean, total_safe_clean = compute_llama_guard(dataset_advbench_clean_seeds)
    print("Loading llama guard results for TAP learned jailbreaks...")
    total_tap, total_safe_tap = compute_llama_guard(dataset_gap_gpt35_advbench)
    print("Loading llama guard results for GAP learned jailbreaks...")
    total_gap, total_safe_gap = compute_llama_guard(dataset_gap_gpt35_advbench)
    print("Loading llama guard results for GCG Suffixes...")
    total_gcg, total_safe_gcg = compute_llama_guard(dataset_advbench_with_gcg_suffixes)
    print("Loading llama guard results for GPTFuzzer Templates...")
    total_gptfuzzer, total_safe_gptfuzzer = compute_llama_guard(dataset_advbench_with_gptfuzzer_templates)

    # Dump all counts
    with open('llama_guard_results.csv', 'w', newline='') as csvfile:
        fieldnames = ['Dataset', 'Total', 'Safe']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerow({'Dataset': 'Clean Seeds', 'Total': total_clean, 'Safe': total_safe_clean})
        writer.writerow({'Dataset': 'TAP Learned Jailbreaks', 'Total': total_tap, 'Safe': total_safe_tap})
        writer.writerow({'Dataset': 'GAP Learned Jailbreaks', 'Total': total_gap, 'Safe': total_safe_gap})
        writer.writerow({'Dataset': 'GCG Suffixes', 'Total': total_gcg, 'Safe': total_safe_gcg})
        writer.writerow({'Dataset': 'GPTFuzzer Templates', 'Total': total_gptfuzzer, 'Safe': total_safe_gptfuzzer})