import logging
import warnings

import easyjailbreak
from easyjailbreak.models.model_base import BlackBoxModelBase
import time
import json
import boto3
import backoff
from transformers import LlamaTokenizer, AutoTokenizer  




class MistralModel(BlackBoxModelBase):
    def __init__(self, model_name: str, generation_config=None):
        """
        Initializes the OpenAI model with necessary parameters.
        :param str model_name: The name of the model to use.
        :param str api_keys: API keys for accessing the OpenAI service.
        :param str template_name: The name of the conversation template, defaults to 'chatgpt'.
        :param dict generation_config: Configuration settings for generation, defaults to an empty dictionary.
        """
        self.client = boto3.client(service_name='bedrock-runtime', region_name="us-west-2") # region_name="us-east-1")
        self.generation_config = generation_config if generation_config is not None else {}
        self.system_message = ""
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-Large-Instruct-2407") #"mistralai/Mistral-7B-v0.1")

    def set_system_message(self, system_message: str):
        """
        Sets a system message for the conversation.
        :param str system_message: The system message to set.
        """
        self.system_message = system_message # self.conversation.system_message = system_message

    def _generate_request_body(self, prompt):
        print("\n\nPROMPT TO SUBMIT\n{}\n\n########################".format(prompt))
        body = json.dumps({
            "prompt": prompt, # "prompt": f"<s>[INST] {prompt} [/INST]",
            "max_tokens": self.generation_config["max_n_tokens"],
            "temperature": self.generation_config["temperature"],
            "top_p": self.generation_config["top_p"],
        })
        return body

    def _extract_completion(self, response):
        response_body = json.loads(response.get("body").read())
        return response_body.get('outputs')[0].get('text')

    def generate_mistral_prompt(self, messages):
        chat = []
        for ii, mm in enumerate(messages):
            if mm[0] == 'USER':
                if ii == 0:
                    print("\nSystem Message: {}\n".format(self.system_message))
                    chat.append({"role": "user", "content": f"{self.system_message}\n{mm[1]}"})
                else:
                    chat.append({"role": "user", "content": mm[1]})

            else:
                chat.append({"role": "assistant", "content": mm[1]})

        prompt = self.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
        return prompt.strip()[:-len("</s>")].strip()

    def get_response(self, messages):
        @backoff.on_exception(
            backoff.expo,
            (
                    self.client.exceptions.ServiceQuotaExceededException,
                    self.client.exceptions.ModelTimeoutException,
                    self.client.exceptions.ModelNotReadyException,
                    self.client.exceptions.ThrottlingException,
            ),
        )
        def _submit_request(client, body, model_name):
            accept = 'application/json'
            contentType = 'application/json'
            response = client.invoke_model(body=body, modelId=model_name, accept=accept, contentType=contentType)
            return response

        t1 = time.time()
        prompt = self.generate_mistral_prompt(messages)
        body = self._generate_request_body(prompt)
        response = _submit_request(self.client, body, self.model_name)
        print(f"Bedrock inference took {time.time() - t1} seconds")
        return self._extract_completion(response)

    def generate(self, messages, clear_old_history=True, **kwargs):
        """
        Generates a response based on messages that include conversation history.
        :param list[str]|str messages: A list of messages or a single message string.
                                       User and assistant messages should alternate.
        :param bool clear_old_history: If True, clears the old conversation history before adding new messages.
        :return str: The response generated by the OpenAI model based on the conversation history.
        """
        # if clear_old_history:
        #     self.conversation.messages = []
        if isinstance(messages, str):
            messages = [messages]
        roles = ['USER', 'ASSISTANT']
        conv = []
        for index, message in enumerate(messages):
            conv.append([roles[index % 2], message])

        # Debugging statements for logging
        print("Conv: {}".format(conv))
        
        response = self.get_response(conv)
        print("\nResponse: {}\n\n".format(response))
        return response

    def batch_generate(self, conversations, **kwargs):
        """
        Generates responses for multiple conversations in a batch.
        :param list[list[str]]|list[str] conversations: A list of conversations, each as a list of messages.
        :return list[str]: A list of responses for each conversation.
        """
        responses = []
        for conversation in conversations:
            if isinstance(conversation, str):
                warnings.warn('For batch generation based on several conversations, provide a list[str] for each conversation. '
                              'Using list[list[str]] will avoid this warning.')
            responses.append(self.generate(conversation, **kwargs))
        return responses